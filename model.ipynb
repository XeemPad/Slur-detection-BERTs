{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55029acc",
   "metadata": {},
   "source": [
    "# Задача классификации наличия матов в тексте отзыва"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c87abf",
   "metadata": {},
   "source": [
    "Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387de762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "import torch\n",
    "\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "    \n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08ff393",
   "metadata": {},
   "source": [
    "### Базовый класс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa769bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class BaseTextClassifier(ABC):\n",
    "    def __init__(self, verbose=True, threshold=0.5):\n",
    "        self.threshold = threshold\n",
    "        self.is_fitted = False\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    @abstractmethod\n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod \n",
    "    def predict(self, X):\n",
    "        pass\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        # По умолчанию - заглушка\n",
    "        predictions = self.predict(X)\n",
    "        probs = np.zeros((len(X), 2))\n",
    "        probs[predictions == 0, 0] = 1.0\n",
    "        probs[predictions == 1, 1] = 1.0\n",
    "        return probs\n",
    "    \n",
    "    def evaluate(self, X, y, model_name=\"Model\"):\n",
    "        \"\"\"Стандартная оценка модели\"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        f1 = f1_score(y, y_pred)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"\\n{model_name} F1-score на validation: {f1:.4f}\")\n",
    "            print(\"\\nClassification Report:\")\n",
    "            print(classification_report(y, y_pred))\n",
    "            print(\"\\nConfusion Matrix:\")\n",
    "            print(confusion_matrix(y, y_pred))\n",
    "        \n",
    "        return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ce14fb",
   "metadata": {},
   "source": [
    "## Извлечение данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afa0e2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "train_df = pd.read_csv(f'{data_dir}/train.csv')\n",
    "test_df = pd.read_csv(f'{data_dir}/test.csv')\n",
    "# sample_submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c84dce",
   "metadata": {},
   "source": [
    "Разбиение на X и Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5da75f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : (192127,) (192127,)\n",
      "Validation : (48032,) (48032,)\n",
      "\n",
      "Распределение классов в train: label\n",
      "0    168369\n",
      "1     23758\n",
      "Name: count, dtype: int64\n",
      "Распределение классов в val: label\n",
      "0    42092\n",
      "1     5940\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target_column = \"label\"\n",
    "np.random.seed(seed)\n",
    "\n",
    "test_size = 0.2\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_df['text'], \n",
    "    train_df[target_column], \n",
    "    test_size=test_size, \n",
    "    random_state=seed,\n",
    "    stratify=train_df[target_column]\n",
    ")\n",
    "\n",
    "print(f\"Train : {X_train.shape} {y_train.shape}\")\n",
    "print(f\"Validation : {X_val.shape} {y_val.shape}\")\n",
    "print(f\"\\nРаспределение классов в train: {y_train.value_counts()}\")\n",
    "print(f\"Распределение классов в val: {y_val.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf1ca5a",
   "metadata": {},
   "source": [
    "### Просмотр данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f43e640a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размеры данных:\n",
      "Train: (240159, 3)\n",
      "Test: (60040, 2)\n",
      "\n",
      "Распределение классов в train:\n",
      "label\n",
      "0    210461\n",
      "1     29698\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Размеры данных:\")\n",
    "print(f\"Train: {train_df.shape}\")\n",
    "print(f\"Test: {test_df.shape}\")\n",
    "print(\"\\nРаспределение классов в train:\")\n",
    "print(train_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef2ad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)  # Показать полную ширину колонок (чтобы видеть отзыв полностью)\n",
    "pd.set_option('display.width', None)  # Убрать ограничение по ширине\n",
    "\n",
    "\n",
    "# Извлечём первые 10 строк с меткой 0 \n",
    "train_df[train_df['label'] == 0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f1760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Извлечём первые 10 строк с меткой 1\n",
    "train_df[train_df['label'] == 1].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c45776b",
   "metadata": {},
   "source": [
    "## Бейслайны"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3c3554",
   "metadata": {},
   "source": [
    "### Частотный бейслайн"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0e5f04",
   "metadata": {},
   "source": [
    "**Частотный baseline** - предсказывает пропорционально распределению классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a04eb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrequencyBaseline(BaseTextClassifier):\n",
    "    def __init__(self, verbose=True):\n",
    "        super().__init__(verbose=verbose)\n",
    "        self.class_1_prob = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Запоминаем долю класса 1 в тренировочных данных\n",
    "        self.class_1_prob = y.mean()\n",
    "        if self.verbose:\n",
    "            print(f\"Доля класса 1 в train: {self.class_1_prob:.3f}\")\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Модель не обучена!\")\n",
    "            \n",
    "        # Случайно предсказываем с вероятностью, равной доле класса 1\n",
    "        np.random.seed(seed)  # Для воспроизводимости\n",
    "        predictions = np.random.binomial(1, self.class_1_prob, size=len(X))\n",
    "        return predictions\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Модель не обучена!\")\n",
    "            \n",
    "        # Возвращаем вероятности на основе частоты класса\n",
    "        probs = np.zeros((len(X), 2))\n",
    "        probs[:, 0] = 1 - self.class_1_prob  # Класс 0\n",
    "        probs[:, 1] = self.class_1_prob      # Класс 1\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adf3add7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля класса 1 в train: 0.124\n"
     ]
    }
   ],
   "source": [
    "# Создаем и тестируем модель\n",
    "baseline_model = FrequencyBaseline()\n",
    "_ =baseline_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21789df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Frequency Baseline F1-score на validation: 0.1280\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88     42092\n",
      "           1       0.13      0.13      0.13      5940\n",
      "\n",
      "    accuracy                           0.78     48032\n",
      "   macro avg       0.50      0.50      0.50     48032\n",
      "weighted avg       0.78      0.78      0.78     48032\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[36904  5188]\n",
      " [ 5179   761]]\n"
     ]
    }
   ],
   "source": [
    "# Используем встроенный метод evaluate\n",
    "f1_baseline = baseline_model.evaluate(X_val, y_val, \"Frequency Baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8250b1",
   "metadata": {},
   "source": [
    "### TF-IDF модель + Логистическая регрессия бейслайн"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553b2c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfIdfBaseline(BaseTextClassifier):\n",
    "    def __init__(self, verbose=True):\n",
    "        super().__init__(verbose=verbose)\n",
    "        self.pipeline = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(\n",
    "                max_features=18000,\n",
    "                ngram_range=(1, 2),\n",
    "                min_df=2,\n",
    "                max_df=0.95,\n",
    "                lowercase=True,\n",
    "                token_pattern=r'\\b\\w+\\b'\n",
    "            )),\n",
    "            ('classifier', LogisticRegression(\n",
    "                random_state=seed,\n",
    "                class_weight='balanced',\n",
    "                C=4.0,\n",
    "                max_iter=1000,\n",
    "                verbose=1 if verbose else 0  # Показывает прогресс LogReg\n",
    "            ))\n",
    "        ])\n",
    "    \n",
    "    def preprocess_text(self, texts):\n",
    "        \"\"\"Простая предобработка текста\"\"\"\n",
    "        processed = []\n",
    "        if self.verbose:\n",
    "            print(\"Предобработка текстов...\")\n",
    "            iterator = tqdm(texts, desc=\"Processing texts\")\n",
    "        else:\n",
    "            iterator = texts\n",
    "            \n",
    "        for text in iterator:\n",
    "            text = re.sub(r'[^\\w\\s]', ' ', str(text).lower())\n",
    "            text = re.sub(r'\\s+', ' ', text.strip())\n",
    "            processed.append(text)\n",
    "        return processed\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.verbose:\n",
    "            print(\"=== Обучение TF-IDF модели ===\")\n",
    "        \n",
    "        # Предобработка с прогрессом\n",
    "        X_processed = self.preprocess_text(X)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Обучение логистической регрессии...\")\n",
    "        self.pipeline.fit(X_processed, y)\n",
    "        \n",
    "        self.is_fitted = True\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"Обучение завершено!\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Модель не обучена!\")\n",
    "        \n",
    "        X_processed = self.preprocess_text(X)\n",
    "        return self.pipeline.predict(X_processed)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Модель не обучена!\")\n",
    "            \n",
    "        X_processed = self.preprocess_text(X)\n",
    "        return self.pipeline.predict_proba(X_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80540184",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Создаем и обучаем модель\n",
    "tfidf_model = TfIdfBaseline()\n",
    "_ = tfidf_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f83fd2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предобработка текстов...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 48032/48032 [00:00<00:00, 139746.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF F1-score на validation: 0.8156\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97     42092\n",
      "           1       0.77      0.87      0.82      5940\n",
      "\n",
      "    accuracy                           0.95     48032\n",
      "   macro avg       0.87      0.92      0.89     48032\n",
      "weighted avg       0.95      0.95      0.95     48032\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40504  1588]\n",
      " [  756  5184]]\n"
     ]
    }
   ],
   "source": [
    "# Оценка с использованием встроенного метода\n",
    "f1_tfidf = tfidf_model.evaluate(X_val, y_val, \"TF-IDF\")  # F1 = 0.8156"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e48eca0",
   "metadata": {},
   "source": [
    "## Модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aec4b9c",
   "metadata": {},
   "source": [
    "### Оптимизация TF-IDF через GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8c7233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "class OptimizedTfIdfBaseline(BaseTextClassifier):\n",
    "    def __init__(self, verbose=True):\n",
    "        super().__init__(verbose=verbose)\n",
    "        \n",
    "        # Базовый pipeline\n",
    "        self.pipeline = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(\n",
    "                min_df=2,\n",
    "                max_df=0.95,\n",
    "                ngram_range=(1, 2), \n",
    "                lowercase=True,\n",
    "                token_pattern=r'\\b\\w+\\b',\n",
    "                sublinear_tf=True  # Помогает с производительностью\n",
    "            )),\n",
    "            ('classifier', LogisticRegression(\n",
    "                random_state=seed,\n",
    "                class_weight='balanced',\n",
    "                solver='liblinear',\n",
    "                verbose=1 if verbose else 0\n",
    "            ))\n",
    "        ])\n",
    "        \n",
    "        # Ограниченный поиск параметров\n",
    "        self.param_grid = {\n",
    "            'tfidf__max_features': [20000, 25000],\n",
    "            'classifier__C': [4.0, 4.5],\n",
    "            'classifier__max_iter': [500, 800]\n",
    "        }\n",
    "        \n",
    "        self.best_model = None\n",
    "        self.grid_search = None\n",
    "    \n",
    "    def preprocess_text(self, texts):\n",
    "        \"\"\"Простая предобработка текста\"\"\"\n",
    "        processed = []\n",
    "        if self.verbose:\n",
    "            print(\"Предобработка текстов...\")\n",
    "            iterator = tqdm(texts, desc=\"Processing texts\")\n",
    "        else:\n",
    "            iterator = texts\n",
    "            \n",
    "        for text in iterator:\n",
    "            text = re.sub(r'[^\\w\\s]', ' ', str(text).lower())\n",
    "            text = re.sub(r'\\s+', ' ', text.strip())\n",
    "            processed.append(text)\n",
    "        return processed\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.verbose:\n",
    "            print(\"=== Grid Search для TF-IDF модели ===\")\n",
    "        \n",
    "        # Предобработка\n",
    "        X_processed = self.preprocess_text(X)\n",
    "        \n",
    "        # Настройка GridSearchCV с экономией памяти\n",
    "        cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=seed)\n",
    "        \n",
    "        if self.verbose:\n",
    "            total_combinations = 1\n",
    "            for param_values in self.param_grid.values():\n",
    "                total_combinations *= len(param_values)\n",
    "            print(f\"Всего комбинаций: {total_combinations}\")\n",
    "            print(f\"С 2-fold CV: {total_combinations * 2} обучений\")\n",
    "        \n",
    "        # GridSearchCV с ограничениями\n",
    "        self.grid_search = GridSearchCV(\n",
    "            self.pipeline,\n",
    "            self.param_grid,\n",
    "            cv=cv,\n",
    "            scoring='f1',\n",
    "            n_jobs=11,  # Ограничиваем процессы\n",
    "            verbose=1 if self.verbose else 0\n",
    "        )\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Запуск Grid Search...\")\n",
    "        \n",
    "        # Обучение\n",
    "        self.grid_search.fit(X_processed, y)\n",
    "        \n",
    "        # Сохраняем лучшую модель\n",
    "        self.best_model = self.grid_search.best_estimator_\n",
    "        self.is_fitted = True\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"\\nGrid Search завершен!\")\n",
    "            print(f\"Лучший F1-score (CV): {self.grid_search.best_score_:.4f}\")\n",
    "            print(f\"Лучшие параметры:\")\n",
    "            for param, value in self.grid_search.best_params_.items():\n",
    "                print(f\"  {param}: {value}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Модель не обучена!\")\n",
    "        \n",
    "        X_processed = self.preprocess_text(X)\n",
    "        return self.best_model.predict(X_processed)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Модель не обучена!\")\n",
    "            \n",
    "        X_processed = self.preprocess_text(X)\n",
    "        return self.best_model.predict_proba(X_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5796a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запускаем оптимизированную модель\n",
    "optimized_model = OptimizedTfIdfBaseline()\n",
    "_ = optimized_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2dcd5d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предобработка текстов...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 48032/48032 [00:00<00:00, 125644.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimized TF-IDF F1-score на validation: 0.8086\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97     42092\n",
      "           1       0.76      0.87      0.81      5940\n",
      "\n",
      "    accuracy                           0.95     48032\n",
      "   macro avg       0.87      0.91      0.89     48032\n",
      "weighted avg       0.95      0.95      0.95     48032\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40428  1664]\n",
      " [  779  5161]]\n",
      "Улучшение по сравнению с базовым TF-IDF: -0.0017\n"
     ]
    }
   ],
   "source": [
    "# Оценка\n",
    "f1_optimized = optimized_model.evaluate(X_val, y_val, \"Optimized TF-IDF\")\n",
    "print(f\"Улучшение по сравнению с базовым TF-IDF: {f1_optimized - f1_tfidf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cf3699",
   "metadata": {},
   "source": [
    "### DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d994ca68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBertBaseline(BaseTextClassifier):\n",
    "    def __init__(self, verbose=True):\n",
    "        super().__init__(verbose=verbose)\n",
    "\n",
    "        self.device = self._get_best_device(verbose)    \n",
    "        if verbose:\n",
    "            print(f\"Устройство: {self.device}\")    \n",
    "        \n",
    "        # Используем русский DistilBERT\n",
    "        self.model_name = \"distilbert-base-multilingual-cased\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.model_name, \n",
    "            num_labels=2\n",
    "        ).to(self.device)  # Переносим модель на нужное устройство\n",
    "\n",
    "    def _get_best_device(self, verbose):\n",
    "        \"\"\"Определяет лучшее доступное устройство\"\"\"\n",
    "                \n",
    "        # Проверяем NVIDIA CUDA\n",
    "        if torch.cuda.is_available():\n",
    "            if verbose:\n",
    "                print(\"Используем NVIDIA GPU (CUDA)\")\n",
    "            return torch.device('cuda')\n",
    "        \n",
    "        # Fallback на CPU\n",
    "        if verbose:\n",
    "            print(\"Используем CPU\")\n",
    "        return torch.device('cpu')    \n",
    "    \n",
    "\n",
    "    def tokenize_texts(self, texts):\n",
    "        return self.tokenizer(\n",
    "            list(texts),\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=128,  # Короткие тексты\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.verbose:\n",
    "            print(f\"Распределение классов: {np.bincount(y)}\")\n",
    "        \n",
    "        # Создаем Dataset\n",
    "        train_encodings = self.tokenize_texts(X)\n",
    "        \n",
    "        class TextDataset(torch.utils.data.Dataset):\n",
    "            def __init__(self, encodings, labels):\n",
    "                self.encodings = encodings\n",
    "                self.labels = labels\n",
    "            \n",
    "            def __getitem__(self, idx):\n",
    "                item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "                item['labels'] = torch.tensor(self.labels[idx])\n",
    "                return item\n",
    "            \n",
    "            def __len__(self):\n",
    "                return len(self.labels)\n",
    "        \n",
    "        train_dataset = TextDataset(train_encodings, y.tolist())\n",
    "        \n",
    "        # Настройки обучения\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='./results',\n",
    "            num_train_epochs=5,\n",
    "            per_device_train_batch_size=8,\n",
    "\n",
    "\n",
    "            learning_rate=3e-5,\n",
    "            warmup_steps=50,\n",
    "            weight_decay=0.1,\n",
    "\n",
    "            logging_steps=25,\n",
    "            save_strategy=\"no\",\n",
    "\n",
    "            adam_epsilon=1e-6,\n",
    "            max_grad_norm=1.0,\n",
    "\n",
    "            logging_dir='./logs',\n",
    "        )\n",
    "        \n",
    "        # Добавляем балансировку классов вручную\n",
    "        from sklearn.utils.class_weight import compute_class_weight\n",
    "        \n",
    "        class_weights = compute_class_weight(\n",
    "            'balanced',\n",
    "            classes=np.unique(y),\n",
    "            y=y\n",
    "        )\n",
    "\n",
    "        # Custom Trainer с взвешенной функцией потерь\n",
    "        class WeightedTrainer(Trainer):\n",
    "            def __init__(self, device, class_weights, *args, **kwargs):\n",
    "                super().__init__(*args, **kwargs)\n",
    "                self.device = device\n",
    "                self.class_weights = class_weights\n",
    "            \n",
    "            def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "                labels = inputs.get(\"labels\")\n",
    "                outputs = model(**inputs)\n",
    "                logits = outputs.get('logits')\n",
    "                \n",
    "                # Взвешенная CrossEntropy\n",
    "                loss_fct = torch.nn.CrossEntropyLoss(\n",
    "                    weight=torch.tensor(class_weights, dtype=torch.float).to(self.device)\n",
    "                )\n",
    "                loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "                \n",
    "                return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Веса классов: {dict(zip(np.unique(y), class_weights))}\")\n",
    "\n",
    "        # Обучение\n",
    "        trainer = WeightedTrainer(\n",
    "            device=self.device,\n",
    "            class_weights=class_weights,\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def predict(self, X):\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Модель не обучена!\")\n",
    "        \n",
    "        # Токенизация входных данных\n",
    "        inputs = self.tokenize_texts(X)\n",
    "        \n",
    "        # Предсказание\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            # Используем более мягкий порог для лучшего recall\n",
    "            probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            \n",
    "            threshold = 0.4  # Снижаем порог для лучшего recall класса 1\n",
    "            predicted_labels = (probabilities[:, 1] > threshold).long()\n",
    "        \n",
    "        return predicted_labels.cpu().numpy()\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Модель не обучена!\")\n",
    "        \n",
    "        # Токенизация входных данных\n",
    "        inputs = self.tokenize_texts(X)\n",
    "        \n",
    "        # Предсказание\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        return probabilities.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3bfffa06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используем CPU\n",
      "Устройство: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Распределение классов: [882 118]\n",
      "Веса классов: {np.int64(0): np.float64(0.5668934240362812), np.int64(1): np.float64(4.237288135593221)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emil/Desktop/Projects/WBtech/Contests/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  7/625 00:16 < 33:25, 0.31 it/s, Epoch 0.05/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m distilbert_model = DistilBertBaseline()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m _ = \u001b[43mdistilbert_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Пробуем на маленькой выборке\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 127\u001b[39m, in \u001b[36mDistilBertBaseline.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# Обучение\u001b[39;00m\n\u001b[32m    119\u001b[39m trainer = WeightedTrainer(\n\u001b[32m    120\u001b[39m     device=\u001b[38;5;28mself\u001b[39m.device,\n\u001b[32m    121\u001b[39m     class_weights=class_weights,\n\u001b[32m   (...)\u001b[39m\u001b[32m    124\u001b[39m     train_dataset=train_dataset,\n\u001b[32m    125\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[38;5;28mself\u001b[39m.is_fitted = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/WBtech/Contests/.venv/lib/python3.13/site-packages/transformers/trainer.py:2240\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2238\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2239\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/WBtech/Contests/.venv/lib/python3.13/site-packages/transformers/trainer.py:2555\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2548\u001b[39m context = (\n\u001b[32m   2549\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2550\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2551\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2552\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2553\u001b[39m )\n\u001b[32m   2554\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2555\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2557\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2558\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2559\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2560\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2561\u001b[39m ):\n\u001b[32m   2562\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2563\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/WBtech/Contests/.venv/lib/python3.13/site-packages/transformers/trainer.py:3745\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3742\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3744\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3745\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3747\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3749\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3750\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3751\u001b[39m ):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 104\u001b[39m, in \u001b[36mDistilBertBaseline.fit.<locals>.WeightedTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, **kwargs)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs, return_outputs=\u001b[38;5;28;01mFalse\u001b[39;00m, **kwargs):\n\u001b[32m    103\u001b[39m     labels = inputs.get(\u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m     logits = outputs.get(\u001b[33m'\u001b[39m\u001b[33mlogits\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    107\u001b[39m     \u001b[38;5;66;03m# Взвешенная CrossEntropy\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/WBtech/Contests/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/WBtech/Contests/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/WBtech/Contests/.venv/lib/python3.13/site-packages/transformers/models/distilbert/modeling_distilbert.py:920\u001b[39m, in \u001b[36mDistilBertForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    912\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    913\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m    914\u001b[39m \u001b[33;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[32m    915\u001b[39m \u001b[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[32m    916\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m    917\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    918\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m--> \u001b[39m\u001b[32m920\u001b[39m distilbert_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdistilbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    929\u001b[39m hidden_state = distilbert_output[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[32m    930\u001b[39m pooled_output = hidden_state[:, \u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/WBtech/Contests/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/WBtech/Contests/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/WBtech/Contests/.venv/lib/python3.13/site-packages/transformers/models/distilbert/modeling_distilbert.py:739\u001b[39m, in \u001b[36mDistilBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    734\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._use_sdpa \u001b[38;5;129;01mand\u001b[39;00m head_mask_is_none \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions:\n\u001b[32m    735\u001b[39m         attention_mask = _prepare_4d_attention_mask_for_sdpa(\n\u001b[32m    736\u001b[39m             attention_mask, embeddings.dtype, tgt_len=input_shape[\u001b[32m1\u001b[39m]\n\u001b[32m    737\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m739\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    740\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    741\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    743\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    744\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    745\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/WBtech/Contests/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/WBtech/Contests/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/WBtech/Contests/.venv/lib/python3.13/site-packages/transformers/models/distilbert/modeling_distilbert.py:544\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    536\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    537\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    538\u001b[39m         hidden_state,\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         output_attentions,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m    543\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m544\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    551\u001b[39m hidden_state = layer_outputs[-\u001b[32m1\u001b[39m]\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/WBtech/Contests/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/WBtech/Contests/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/WBtech/Contests/.venv/lib/python3.13/site-packages/transformers/models/distilbert/modeling_distilbert.py:488\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, x, attn_mask, head_mask, output_attentions)\u001b[39m\n\u001b[32m    485\u001b[39m sa_output = \u001b[38;5;28mself\u001b[39m.sa_layer_norm(sa_output + x)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[32m    487\u001b[39m \u001b[38;5;66;03m# Feed Forward Network\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m ffn_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mffn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa_output\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[32m    489\u001b[39m ffn_output: torch.Tensor = \u001b[38;5;28mself\u001b[39m.output_layer_norm(ffn_output + sa_output)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[32m    491\u001b[39m output = (ffn_output,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/WBtech/Contests/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/WBtech/Contests/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/WBtech/Contests/.venv/lib/python3.13/site-packages/transformers/models/distilbert/modeling_distilbert.py:422\u001b[39m, in \u001b[36mFFN.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mff_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/WBtech/Contests/.venv/lib/python3.13/site-packages/transformers/pytorch_utils.py:253\u001b[39m, in \u001b[36mapply_chunking_to_forward\u001b[39m\u001b[34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[39m\n\u001b[32m    250\u001b[39m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(output_chunks, dim=chunk_dim)\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/WBtech/Contests/.venv/lib/python3.13/site-packages/transformers/models/distilbert/modeling_distilbert.py:425\u001b[39m, in \u001b[36mFFN.ff_chunk\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    424\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mff_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m425\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlin1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    426\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.activation(x)\n\u001b[32m    427\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.lin2(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/WBtech/Contests/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/WBtech/Contests/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/WBtech/Contests/.venv/lib/python3.13/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "distilbert_model = DistilBertBaseline()\n",
    "_ = distilbert_model.fit(X_train[:1000], y_train[:1000])  # Пробуем на маленькой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30be2a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DistilBERT F1-score на validation: 0.6786\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96       430\n",
      "           1       0.90      0.54      0.68        70\n",
      "\n",
      "    accuracy                           0.93       500\n",
      "   macro avg       0.92      0.77      0.82       500\n",
      "weighted avg       0.93      0.93      0.92       500\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[426   4]\n",
      " [ 32  38]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Attempt 1: 0.6786\n",
    "Attempt 2:\n",
    "'''\n",
    "f1_distilbert = distilbert_model.evaluate(X_val[:200], y_val[:200], \"DistilBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc1df83",
   "metadata": {},
   "source": [
    "### RuBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96995b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuBert(BaseTextClassifier):\n",
    "    def __init__(self, verbose=True):\n",
    "        super().__init__(verbose=verbose)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        if verbose:\n",
    "            print(f\"Устройство: {self.device}\")\n",
    "            print(f\"GPU доступно: {torch.cuda.device_count()} устройств\")\n",
    "\n",
    "        self.model_name = \"DeepPavlov/rubert-base-cased\"\n",
    "        # self.model_name = \"ai-forever/ruBert-base\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.model_name, num_labels=2\n",
    "        )\n",
    "        \n",
    "        if torch.cuda.device_count() > 1:\n",
    "            if verbose:\n",
    "                print(f\"Используем {torch.cuda.device_count()} GPU с DataParallel\")\n",
    "            self.model = torch.nn.DataParallel(self.model)\n",
    "            self.model.to(self.device)\n",
    "            # Сохраняем оригинальную модель для доступа к методам\n",
    "            self.base_model = self.model.module\n",
    "        else:\n",
    "            self.model.to(self.device)\n",
    "            self.base_model = self.model\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.verbose:\n",
    "            print(f\"Размер датасета: {len(X)} образцов\")\n",
    "\n",
    "        class TextDataset(torch.utils.data.Dataset):\n",
    "            def __init__(self, texts, labels, tokenizer):\n",
    "                self.texts = texts.tolist() if hasattr(texts, 'tolist') else texts\n",
    "                self.labels = labels.tolist() if hasattr(labels, 'tolist') else labels\n",
    "                self.tokenizer = tokenizer\n",
    "\n",
    "            def __getitem__(self, idx):\n",
    "                text = str(self.texts[idx])\n",
    "                encoding = self.tokenizer(\n",
    "                    text, truncation=True, padding='max_length',\n",
    "                    max_length=96, return_tensors=\"pt\"\n",
    "                )\n",
    "                return {\n",
    "                    'input_ids': encoding['input_ids'].flatten(),\n",
    "                    'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                    'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "                }\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.labels)\n",
    "\n",
    "        train_dataset = TextDataset(X, y, self.tokenizer)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='./results',\n",
    "            \n",
    "            num_train_epochs=6,\n",
    "            per_device_train_batch_size=96,\n",
    "            \n",
    "            dataloader_num_workers=0,\n",
    "            dataloader_pin_memory=False,\n",
    "            fp16=True,\n",
    "            \n",
    "            learning_rate=8e-6,\n",
    "            warmup_steps=500,\n",
    "            weight_decay=0.01,\n",
    "            \n",
    "            logging_steps=50,\n",
    "            save_strategy=\"no\",\n",
    "            remove_unused_columns=False,\n",
    "            report_to=\"none\"\n",
    "        )\n",
    "\n",
    "        # Балансировка классов\n",
    "        from sklearn.utils.class_weight import compute_class_weight\n",
    "        class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "\n",
    "        class WeightedTrainer(Trainer):\n",
    "            def __init__(self, *args, **kwargs):\n",
    "                super().__init__(*args, **kwargs)\n",
    "                self.step = 0\n",
    "            \n",
    "            def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "                labels = inputs.get(\"labels\")\n",
    "                \n",
    "                model_device = next(model.parameters()).device\n",
    "                inputs = {k: v.to(model_device) for k, v in inputs.items()}\n",
    "                \n",
    "                outputs = model(**inputs)\n",
    "\n",
    "                loss_fct = torch.nn.CrossEntropyLoss(\n",
    "                    weight=torch.tensor(class_weights, dtype=torch.float).to(model_device)\n",
    "                )\n",
    "                loss = loss_fct(outputs.logits.view(-1, 2), labels.view(-1))\n",
    "                self.step += 1\n",
    "                if self.step % 50 == 0:\n",
    "                    print(f\"iter {self.step}. loss: {loss}\")\n",
    "                return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Веса классов: {dict(zip(np.unique(y), class_weights))}\")\n",
    "\n",
    "        trainer = WeightedTrainer(model=self.model, args=training_args, train_dataset=train_dataset)\n",
    "        trainer.train()\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Модель не обучена!\")\n",
    "\n",
    "        batch_size = 64\n",
    "        predictions = []\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(X), batch_size):\n",
    "                batch_texts = X[i:i+batch_size].tolist() if hasattr(X, 'tolist') else X[i:i+batch_size]\n",
    "                inputs = self.tokenizer(\n",
    "                    batch_texts, truncation=True, padding=True,\n",
    "                    max_length=96, return_tensors=\"pt\"\n",
    "                ).to(self.device)\n",
    "\n",
    "                outputs = self.model(**inputs)\n",
    "                probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "                batch_preds = (probs[:, 1] > self.threshold).long()\n",
    "                predictions.extend(batch_preds.cpu().numpy())\n",
    "\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Модель не обучена!\")\n",
    "\n",
    "        batch_size = 64\n",
    "        probabilities = []\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(X), batch_size):\n",
    "                batch_texts = X[i:i+batch_size].tolist() if hasattr(X, 'tolist') else X[i:i+batch_size]\n",
    "                inputs = self.tokenizer(\n",
    "                    batch_texts, truncation=True, padding=True,\n",
    "                    max_length=96, return_tensors=\"pt\"\n",
    "                ).to(self.device)\n",
    "\n",
    "                outputs = self.model(**inputs)\n",
    "                batch_probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "                probabilities.extend(batch_probs.cpu().numpy())\n",
    "\n",
    "        return np.array(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ca4bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Устройство: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruBert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер датасета: 2000 образцов\n",
      "Ожидаемое время: ~0 минут\n",
      "Веса классов: {np.int64(0): np.float64(0.5614823133071308), np.int64(1): np.float64(4.566210045662101)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  5/500 00:10 < 29:51, 0.28 it/s, Epoch 0.02/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m rubert_model = RuBertBaseline()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m _ = \u001b[43mrubert_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 83\u001b[39m, in \u001b[36mRuBertBaseline.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m     80\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mВеса классов: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(np.unique(y),\u001b[38;5;250m \u001b[39mclass_weights))\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     82\u001b[39m trainer = WeightedTrainer(model=\u001b[38;5;28mself\u001b[39m.model, args=training_args, train_dataset=train_dataset)\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;28mself\u001b[39m.is_fitted = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/WBtech/Contests/.venv/lib/python3.13/site-packages/transformers/trainer.py:2240\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2238\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2239\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/WBtech/Contests/.venv/lib/python3.13/site-packages/transformers/trainer.py:2555\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2548\u001b[39m context = (\n\u001b[32m   2549\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2550\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2551\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2552\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2553\u001b[39m )\n\u001b[32m   2554\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2555\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2557\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2558\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2559\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2560\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2561\u001b[39m ):\n\u001b[32m   2562\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2563\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/WBtech/Contests/.venv/lib/python3.13/site-packages/transformers/trainer.py:3791\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3788\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   3789\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3791\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3793\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/WBtech/Contests/.venv/lib/python3.13/site-packages/accelerate/accelerator.py:2473\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2471\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2472\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2473\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/WBtech/Contests/.venv/lib/python3.13/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/WBtech/Contests/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/WBtech/Contests/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "rubert_model = RuBert()\n",
    "_ = rubert_model.fit(X_train[:2000], y_train[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c720326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_rubert = rubert_model.evaluate(X_val[:500], y_val[:500], \"RuBERT Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23e71ef",
   "metadata": {},
   "source": [
    "### AlBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2404c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Albert(BaseTextClassifier):\n",
    "    def __init__(self, verbose=True, **kwargs):\n",
    "        super().__init__(verbose=verbose, **kwargs)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        if verbose:\n",
    "            print(f\"Устройство: {self.device}\")\n",
    "            print(f\"GPU доступно: {torch.cuda.device_count()} устройств\")\n",
    "\n",
    "        self.model_name = \"ai-forever/ru-en-RoSBERTa\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.model_name, num_labels=2\n",
    "        )\n",
    "    \n",
    "        if torch.cuda.device_count() > 1:\n",
    "            if verbose:\n",
    "                print(f\"Используем {torch.cuda.device_count()} GPU с DataParallel\")\n",
    "            self.model = torch.nn.DataParallel(self.model)\n",
    "            self.model.to(self.device)\n",
    "            # Сохраняем оригинальную модель для доступа к методам\n",
    "            self.base_model = self.model.module\n",
    "        else:\n",
    "            self.model.to(self.device)\n",
    "            self.base_model = self.model\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        if self.verbose:\n",
    "            print(f\"Размер датасета: {len(X)} образцов\")\n",
    "\n",
    "        class TextDataset(torch.utils.data.Dataset):\n",
    "            def __init__(self, texts, labels, tokenizer):\n",
    "                self.texts = texts.tolist() if hasattr(texts, 'tolist') else texts\n",
    "                self.labels = labels.tolist() if hasattr(labels, 'tolist') else labels\n",
    "                self.tokenizer = tokenizer\n",
    "\n",
    "            def __getitem__(self, idx):\n",
    "                text = str(self.texts[idx])\n",
    "                encoding = self.tokenizer(\n",
    "                    text, truncation=True, padding='max_length',\n",
    "                    max_length=96, return_tensors=\"pt\"\n",
    "                )\n",
    "                return {\n",
    "                    'input_ids': encoding['input_ids'].flatten(),\n",
    "                    'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                    'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "                }\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.labels)\n",
    "\n",
    "        train_dataset = TextDataset(X, y, self.tokenizer)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f'{output_dir}/results_albert',\n",
    "            resume_from_checkpoint='/kaggle/input/albert/pytorch/v1-6000it/1/checkpoint-6004',\n",
    "\n",
    "            num_train_epochs=5,\n",
    "            per_device_train_batch_size=64,\n",
    "\n",
    "            dataloader_num_workers=0,\n",
    "            dataloader_pin_memory=False,\n",
    "            fp16=True,\n",
    "\n",
    "            learning_rate=1e-5,  \n",
    "            warmup_steps=500,   \n",
    "            weight_decay=0.02,  \n",
    "\n",
    "            logging_steps=50,\n",
    "            save_strategy=\"no\",\n",
    "            remove_unused_columns=False,\n",
    "            report_to=\"none\"\n",
    "        )\n",
    "\n",
    "        # Балансировка классов\n",
    "        from sklearn.utils.class_weight import compute_class_weight\n",
    "        class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "\n",
    "        class WeightedTrainer(Trainer):\n",
    "            def __init__(self, *args, **kwargs):\n",
    "                super().__init__(*args, **kwargs)\n",
    "                self.step = 0\n",
    "                \n",
    "            def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "                labels = inputs.get(\"labels\")\n",
    "                \n",
    "                model_device = next(model.parameters()).device\n",
    "                inputs = {k: v.to(model_device) for k, v in inputs.items()}\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "                loss_fct = torch.nn.CrossEntropyLoss(\n",
    "                    weight=torch.tensor(class_weights, dtype=torch.float).to(model_device)\n",
    "                )\n",
    "                loss = loss_fct(outputs.logits.view(-1, 2), labels.view(-1))\n",
    "                self.step += 1\n",
    "                print(f\"iter {self.step}. loss: {loss}\")\n",
    "                if self.step % 50 == 0:\n",
    "                    print(f\"iter {self.step}. loss: {loss}\")\n",
    "                return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Веса классов: {dict(zip(np.unique(y), class_weights))}\")\n",
    "\n",
    "        trainer = WeightedTrainer(model=self.model, args=training_args, train_dataset=train_dataset)\n",
    "        trainer.train()\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Модель не обучена!\")\n",
    "\n",
    "        batch_size = 32 \n",
    "        predictions = []\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(X), batch_size):\n",
    "                batch_texts = X[i:i+batch_size].tolist() if hasattr(X, 'tolist') else X[i:i+batch_size]\n",
    "                inputs = self.tokenizer(\n",
    "                    batch_texts, truncation=True, padding=True,\n",
    "                    max_length=96, return_tensors=\"pt\" \n",
    "                ).to(self.device)\n",
    "\n",
    "                outputs = self.model(**inputs)\n",
    "                probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "                batch_preds = (probs[:, 1] > self.threshold).long() \n",
    "                predictions.extend(batch_preds.cpu().numpy())\n",
    "\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Модель не обучена!\")\n",
    "\n",
    "        batch_size = 32\n",
    "        probabilities = []\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(X), batch_size):\n",
    "                batch_texts = X[i:i+batch_size].tolist() if hasattr(X, 'tolist') else X[i:i+batch_size]\n",
    "                inputs = self.tokenizer(\n",
    "                    batch_texts, truncation=True, padding=True,\n",
    "                    max_length=96, return_tensors=\"pt\"\n",
    "                ).to(self.device)\n",
    "\n",
    "                outputs = self.model(**inputs)\n",
    "                batch_probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "                probabilities.extend(batch_probs.cpu().numpy())\n",
    "\n",
    "        return np.array(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95a0ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "albert_model = Albert()\n",
    "_ = albert_model.fit(X_train[:3000], y_train[:3000])  # Больше данных чем для RuBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5ac2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_albert_test = albert_model.evaluate(X_val[:1000], y_val[:1000], \"ALBERT Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da4e8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ОПТИМИЗАЦИЯ THRESHOLD ДЛЯ ALBERT\")\n",
    "\n",
    "# Диапазон threshold для тестирования\n",
    "thresholds = np.round(np.arange(0.4, 0.95, 0.05), 3).tolist()\n",
    "\n",
    "results = []\n",
    "\n",
    "print(f\"Тестируем thresholds: {thresholds}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "verbose = albert_model.verbose\n",
    "albert_model.verbose = False\n",
    "for threshold in thresholds:\n",
    "    # Временно изменяем threshold у модели\n",
    "    albert_model.threshold = threshold\n",
    "    \n",
    "    # Оцениваем с новым threshold\n",
    "    f1 = albert_model.evaluate(\n",
    "        X_val[:1000], \n",
    "        y_val[:1000], \n",
    "        f\"ALBERT (threshold={threshold})\"\n",
    "    )\n",
    "    \n",
    "    # Сохраняем результат\n",
    "    results.append({\n",
    "        'threshold': threshold,\n",
    "        'f1_score': f1\n",
    "    })\n",
    "    \n",
    "    print(f\"Threshold {threshold} -> F1: {f1:.4f}\")\n",
    "\n",
    "best = max(results, key=lambda x: x['f1_score'])\n",
    "print(f\"Best: Threshold={best['threshold']} -> F1={best['f1_score']}\")\n",
    "albert_model.verbose = verbose  # Восстанавливаем исходное состояние\n",
    "\n",
    "# Устанавливаем лучший threshold\n",
    "albert_model.threshold = best['threshold']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91476065",
   "metadata": {},
   "source": [
    "## Выгрузка решения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215eed9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(model, test_data, filename=\"submission.csv\"):\n",
    "    \"\"\"\n",
    "    Создает файл submission для отправки результатов\n",
    "    \n",
    "    Args:\n",
    "        model: обученная модель с методом predict\n",
    "        test_data: DataFrame с тестовыми данными\n",
    "        filename: имя файла для сохранения\n",
    "    \"\"\"\n",
    "    \n",
    "    # Получаем предсказания на тестовых данных\n",
    "    test_predictions = model.predict(test_data['text'])\n",
    "    \n",
    "    # Создаем DataFrame для submission\n",
    "    submission = pd.DataFrame({\n",
    "        'ID': test_data['ID'],  # Используем ID из тестовых данных\n",
    "        'label': test_predictions\n",
    "    })\n",
    "    \n",
    "    # Сохраняем файл\n",
    "    submission.to_csv(filename, index=False)\n",
    "    \n",
    "    print(f\"Файл {filename} создан!\")\n",
    "    print(f\"Размер: {submission.shape}\")\n",
    "    print(f\"Распределение предсказаний:\")\n",
    "    print(submission['label'].value_counts().sort_index())\n",
    "    print(f\"\\nПример submission:\")\n",
    "    print(submission.head(10))\n",
    "    \n",
    "    return submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78cb9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем submission с TF-IDF моделью\n",
    "submission_tfidf = create_submission(\n",
    "    model=tfidf_model, \n",
    "    test_data=test_df, \n",
    "    filename=f\"{data_dir}/tfidf_baseline1_submission.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc0697a",
   "metadata": {},
   "source": [
    "Сохранение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3c4540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def save_model_complete(model, model_name, save_dir=\"saved_models\"):\n",
    "    \"\"\"\n",
    "    Полное сохранение модели с метаданными\n",
    "    \"\"\"\n",
    "    save_path = Path(save_dir) / model_name\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"Сохраняем модель {model_name} в {save_path}\")\n",
    "    \n",
    "    # Сохраняем веса и конфигурацию модели\n",
    "    if hasattr(model.model, 'module'):  # DataParallel case\n",
    "        model.model.module.save_pretrained(save_path / \"model\")\n",
    "    else:\n",
    "        model.model.save_pretrained(save_path / \"model\")\n",
    "    \n",
    "    # Сохраняем токенизатор\n",
    "    model.tokenizer.save_pretrained(save_path / \"tokenizer\")\n",
    "    \n",
    "    # Сохраняем метаданные класса\n",
    "    metadata = {\n",
    "        'class_name': model.__class__.__name__,\n",
    "        'model_name': model.model_name,\n",
    "        'threshold': model.threshold,\n",
    "        'device': str(model.device),\n",
    "        'is_fitted': model.is_fitted,\n",
    "        'verbose': model.verbose\n",
    "    }\n",
    "    \n",
    "    with open(save_path / \"metadata.json\", 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    # Сохраняем состояние оптимизатора (если нужно продолжить обучение)\n",
    "    # torch.save(trainer.state, save_path / \"trainer_state.pt\")\n",
    "    \n",
    "    print(f\"Модель сохранена:\")\n",
    "    print(f\"Веса модели: {save_path / 'model'}\")\n",
    "    print(f\"Токенизатор: {save_path / 'tokenizer'}\")\n",
    "    print(f\"Метаданные: {save_path / 'metadata.json'}\")\n",
    "    \n",
    "    return save_path\n",
    "\n",
    "def load_model_complete(model_class, save_path):\n",
    "    \"\"\"\n",
    "    Полная загрузка модели\n",
    "    \"\"\"\n",
    "    save_path = Path(save_path)\n",
    "    \n",
    "    # Загружаем метаданные\n",
    "    with open(save_path / \"metadata.json\", 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    # Создаем экземпляр класса\n",
    "    model = model_class(verbose=metadata['verbose'])\n",
    "    \n",
    "    # Загружаем веса модели\n",
    "    model.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        save_path / \"model\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # Загружаем токенизатор\n",
    "    model.tokenizer = AutoTokenizer.from_pretrained(save_path / \"tokenizer\")\n",
    "    \n",
    "    # Восстанавливаем состояние\n",
    "    model.threshold = metadata['threshold']\n",
    "    model.is_fitted = metadata['is_fitted']\n",
    "    \n",
    "    print(f\"Модель загружена из {save_path}\")\n",
    "    return model\n",
    "\n",
    "save_path = save_model_complete(rubert_model, \"rubert_best\")\n",
    "# loaded_model = load_model_complete(RuBert, save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
