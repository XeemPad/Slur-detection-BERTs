## Предисловие

В репозитории содержаится простой код для обучения классификатора нецензурных выражений (не обязательно матов) 
в тексте на основе разных решений.

Данные не пренадлежат автору кода и были удалены из репозитория. Сохранённые состояния моделей слишком велики для 
*github* и лежат на *kaggle*.

Лучшие результаты на тестовом датасете (**F1-score = 0.958**) показал *RoSBERTa* от *ai-forever*, который тем не менее
назван AlBERT в связи с путаницей при тестировании разных базовых моделей.

Будучи одним из первых проектов в AI-based NLP, он притягивает внимание к плохой структуре кода, которая плохо 
масштабировалась бы в полевых условиях. Последующие проекты (*Slur Extraction*, *Tech Support Chatbot*) имеют 
более аккуратную структуру и, например, исключают классы моделей и препроцессоров из основного ноутбука

## Условие

В современном цифровом мире создание уважительной и безопасной среды общения является важной задачей. В этом соревновании вам предстоит разработать систему, которая будет автоматически выявлять и модерировать нецензурную лексику в пользовательских текстах. При этом, модерировать токсичность не требуется — ваша задача заключается только в выявлении и замене неприемлемых слов.

### Задача:
Вы будете работать с реальными данными, содержащими отзывы и комментарии пользователей, где могут встречаться нецензурные выражения. Цель — выявить и скрыть нецензурные слова и фразы, заменив их на нейтральные символы, обеспечив тем самым чистоту и безопасность общения на платформе.

Это соревнование моделирует задачи, с которыми сталкиваются системы модерации контента на онлайн-площадках, и ваше решение поможет улучшить пользовательский опыт и повысить безопасность на платформе.

Сможет ли ваша модель точно и эффективно выявить нецензурные выражения, сохранив общий смысл текста? Узнаем в ходе соревнования!

## Описание

Цель соревнования — создать модель, которая сможет предсказывать наличие нецензурной лексики в пользовательских отзывах и комментариях. Вместо того чтобы заменять или скрывать нецензурные слова, задача сводится к бинарной классификации текста: если в нем есть мат, модель должна поставить флаг "1", если мата нет — "0". Основная метрика, по которой будет оцениваться решение, — F1-score.

### Предпосылки

Нецензурная лексика является проблемой для многих онлайн-платформ, где пользователи оставляют отзывы или комментарии. Умение оперативно обнаружить и пометить контент с нецензурными выражениями важно для обеспечения безопасного и комфортного взаимодействия пользователей.

В данной задаче вы будете работать с текстами на русском языке, которые могут содержать как открытые, так и завуалированные формы нецензурных выражений. Задача модели — классифицировать текст, указывая, присутствует ли в нем мат.

### Описание задачи

Ваше решение должно определить, содержит ли текст нецензурные выражения, и выставить соответствующий бинарный флаг:

1 — в тексте есть мат.
0 — в тексте нет мата.
Вам предстоит использовать набор данных с текстовыми комментариями и отзывами, где каждый текст уже помечен флагом о наличии или отсутствии мата. Ваша задача — построить модель, которая сможет эффективно решать эту задачу на новых, ранее невиданных данных.

### Пример:

Оригинальный текст: "Эта юбка полное г…"
Ожидаемый флаг: 1 (есть мат).
Оригинальный текст: "Эта юбка мне не понравилась."
Ожидаемый флаг: 0 (мата нет).
Основная метрика

Решение будет оцениваться с использованием метрики F1-score. Эта метрика учитывает как точность, так и полноту предсказаний, что особенно важно в задачах с несбалансированными классами (когда, например, количество текстов с матом может быть меньше, чем без него).

### Задачи:

Разработать модель для бинарной классификации текстов, которая предсказывает наличие мата.
Оптимизировать модель с фокусом на повышении метрики F1-score.
Обработать различные виды нецензурных выражений, включая завуалированные и сокращенные формы.
Ожидаемый результат
Модель должна точно предсказывать, содержит ли текст нецензурные выражения, позволяя онлайн-платформам быстро выявлять потенциально неприемлемый контент.

## Оценка решения

В данном соревновании ваши решения будут оцениваться с помощью метрики F1-score. Эта метрика сбалансированно учитывает точность (precision) и полноту (recall), что особенно важно при работе с несбалансированными данными, как в случае обнаружения нецензурной лексики в тексте.

### Submission File
Ваше решение должно быть представлено в виде файла CSV с двумя колонками:

id — уникальный идентификатор текста.
label — предсказанный флаг (1 — мат присутствует, 0 — мата нет).

ID, label
2,0
5,1
6,0
etc.

## Описание dataset'ов

В этом разделе представлены файлы данных, которые вам понадобятся для участия в соревновании, а также формат и содержание этих файлов. Здесь вы найдете информацию о том, что содержится в каждом файле, какие колонки используются, что вам необходимо предсказать и другие важные детали.

Файлы:

train.csv — тренировочный набор данных. Этот файл содержит тексты и их метки, которые указывают, присутствует ли в тексте нецензурная лексика.
test.csv — тестовый набор данных. Этот файл содержит тексты, для которых вам необходимо предсказать наличие мата. Метки для этих данных не предоставлены.
sample_submission.csv — пример файла для сабмита. Этот файл показывает правильный формат вашего предсказания для тестовых данных.
Структура колонок в файлах:

ID — уникальный идентификатор примера.
text — текст сообщения, который может содержать или не содержать нецензурную лексику.
label — целевая переменная для тренировочного набора данных:
0 — мат отсутствует.
1 — мат присутствует.

Вы должны предсказать наличие или отсутствие мата в текстах тестового набора данных. Для каждого текста необходимо предсказать флаг:

1 — если текст содержит нецензурную лексику.
0 — если текст чистый.